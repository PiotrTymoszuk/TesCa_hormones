@article{Zhu2022,
abstract = {Purpose:There is an age-related decline in male testosterone production. It is therefore surprising that young men are evaluated for testosterone deficiency with the same cutoff of 300 ng/dL that was developed from samples of older men. Our aim is to describe normative total testosterone levels and age-specific cutoffs for low testosterone levels in men 20 to 44 years old.Materials and Methods:We analyzed the 2011-2016 National Health and Nutrition Examination Surveys, which survey nationally representative samples of United States residents. Men 20 to 44 years old with testosterone levels were included. Men on hormonal medications, with a history of testicular cancer or orchiectomy, and with afternoon/evening laboratory values were excluded. We separated men into 5-year intervals and evaluated the testosterone levels of each age group, and for all men 20 to 44 years old. We used the American Urological Association definition of a "normal testosterone level"(the "middle tertile") to calculate age-specific cutoffs for low testosterone levels.Results:Our final analytic cohort contained 1,486 men. Age-specific middle tertile levels were 409-558 ng/dL (20-24 years old), 413-575 ng/dL (25-29 years old), 359-498 ng/dL (30-34 years old), 352-478 ng/dL (35-39 years old), and 350-473 ng/dL (40-44 years old). Age-specific cutoffs for low testosterone levels were 409, 413, 359, 352, and 350 ng/dL, respectively.Conclusions:Diagnosis of testosterone deficiency has traditionally been performed in an age-indiscriminate manner. However, young men have different testosterone reference ranges than older men. Accordingly, age-specific normative values and cutoffs should be integrated into the evaluation of young men presenting with testosterone deficiency.},
author = {Zhu, Alex and Andino, Juan and Daignault-Newton, Stephanie and Chopra, Zoey and Sarma, Aruna and Dupree, James M.},
doi = {10.1097/JU.0000000000002928},
issn = {1527-3792},
journal = {The Journal of urology},
keywords = {Adult,Aged,Alex Zhu,Humans,Hypogonadism* / drug therapy,James M Dupree,Juan Andino,MEDLINE,Male,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Nutrition Surveys,PubMed Abstract,Reference Values,Testicular Neoplasms* / drug therapy,Testosterone / therapeutic use,United States / epidemiology,Young Adult,doi:10.1097/JU.0000000000002928,pmid:36282060},
month = {dec},
number = {6},
pages = {1295--1301},
pmid = {36282060},
publisher = {J Urol},
title = {{What Is a Normal Testosterone Level for Young Men? Rethinking the 300 ng/dL Cutoff for Testosterone Deficiency in Men 20-44 Years Old}},
url = {https://pubmed.ncbi.nlm.nih.gov/36282060/},
volume = {208},
year = {2022}
}
@article{Milose2012,
abstract = {Testis cancer is one of the few solid organ malignancies for which reliable serum tumor markers are available to help guide disease management. Human chorionic gonadotropin, alpha fetoprotein, and lactate dehydrogenase play crucial roles in diagnosis, staging, prognosis, monitoring treatment response, and surveillance of seminomatous and nonseminomatous germ cell tumors. Herein we discuss the clinical applications of germ cell tumor markers, the limitations of these markers in the management of this disease, and additional serum molecules that have been identified with potential roles as novel germ cell tumor markers. {\textcopyright} 2012 Milose et al, publisher and licensee Dove Medical Press Ltd.},
author = {Milose, Jaclyn C. and Filson, Christopher P. and Weizer, Alon Z. and Hafez, Khaled S. and Montgomery, Jeffrey S.},
doi = {10.2147/OAJU.S15063},
issn = {11791551},
journal = {Open Access Journal of Urology},
keywords = {AFP,Diagnosis,Surveillance,Testicular cancer,Tumor markers,bhCG},
month = {dec},
number = {1},
pages = {1},
pmid = {24198649},
publisher = {Dove Press},
title = {{Role of biochemical markers in testicular cancer: diagnosis, staging, and surveillance}},
url = {/pmc/articles/PMC3818947/ /pmc/articles/PMC3818947/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3818947/},
volume = {4},
year = {2012}
}
@article{Bandeen-roche1997,
abstract = {Quantifying human health and functioning poses significant challenges in many research areas. Commonly in the social and behavioral sciences and increasingly in epidemiologic research, multiple indicators are utilized as responses in lieu of an obvious single measure for an outcome of interest. In this article we study the concomitant latent class model for analyzing such multivariate categorical outcome data. We develop practical theory for reducing and identifying such models. We detail parameter and standard error fitting that parallels standard latent class methodology, thus supplementing the approach proposed by Dayton and Macready. We propose and study diagnostic strategies, exemplifying our methods using physical disability data from an ongoing gerontologic study. Throughout, the focus of our work is on applications for which a primary goal is to study the association between health or functioning and covariates. {\textcopyright} 1997 Taylor & Francis Group, LLC.},
author = {Bandeen-roche, Karen and Miglioretti, Diana L. and Zeger, Scott L. and Rathouz, Paul J.},
doi = {10.1080/01621459.1997.10473658},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Categorical data,Diagnosis,Identifiability,Latent class,Link function,Mixture model},
month = {dec},
number = {440},
pages = {1375--1386},
publisher = {Taylor & Francis Group},
title = {{Latent variable regression for multiple discrete outcomes}},
url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1997.10473658},
volume = {92},
year = {1997}
}
@article{Linzer2011,
abstract = {poLCA is a software package for the estimation of latent class and latent class regression models for polytomous outcome variables, implemented in the R statistical computing environment. Both models can be called using a single simple command line. The basic latent class model is a finite mixture model in which the component distributions are assumed to be multi-way cross-classification tables with all variables mutually independent. The latent class regression model further enables the researcher to estimate the effects of covariates on predicting latent class membership. poLCA uses expectation-maximization and Newton-Raphson algorithms to find maximum likelihood estimates of the model parameters.},
author = {Linzer, Drew A. and Lewis, Jeffrey B.},
doi = {10.18637/jss.v042.i10},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Categorical,Concomitant,Latent class analysis,Latent class regression,Polytomous},
month = {jun},
number = {10},
pages = {1--29},
publisher = {American Statistical Association},
title = {{poLCA: An R package for polytomous variable latent class analysis}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v042i10},
volume = {42},
year = {2011}
}
@article{Graf1999,
abstract = {Prognostic classification schemes have often been used in medical applications, but rarely subjected to a rigorous examination of their adequacy. For survival data, the statistical methodology to assess such schemes consists mainly of a range of ad hoc approaches, and there is an alarming lack of commonly accepted standards in this field. We review these methods and develop measures of inaccuracy which may be calculated in a validation study in order to assess the usefulness of estimated patient-specific survival probabilities associated with a prognostic classification scheme. These measures are meaningful even when the estimated probabilities are misspecified, and asymptotically they are not affected by random censorship. In addition, they can be used to derive R2-type measures of explained residual variation. A breast cancer study will serve for illustration throughout the paper.},
author = {Graf, Erika and Schmoor, Claudia and Sauerbrei, Willi and Schumacher, Martin},
doi = {10.1002/(sici)1097-0258(19990915/30)18:17/18<2529::aid-sim274>3.0.co;2-5},
issn = {0277-6715},
journal = {Statistics in Medicine},
keywords = {Europe PMC,Europe PubMed Central,ORCIDs,REST APIs,abstracts,bioinformatics,biological patents,biomedical journals,biomedical research,citation networks,citation search,clinical guidelines,full text,journal articles,life sciences,literature search,open access,research articles,text mining},
month = {sep},
number = {17-18},
pages = {2529--2545},
pmid = {10474158},
publisher = {John Wiley and Sons Ltd},
title = {{Assessment and comparison of prognostic classification schemes for survival data.}},
url = {https://europepmc.org/article/med/10474158},
volume = {18},
year = {1999}
}
@article{Gerds2006,
abstract = {In survival analysis with censored data the mean squared error of prediction can be estimated by weighted averages of time-dependent residuals. Graf et al. (1999) suggested a robust weighting scheme based on the assumption that the censoring mechanism is independent of the covariates. We show consistency of the estimator. Furthermore, we show that a modified version of this estimator is consistent even when censoring and event times are only conditionally independent given the covariates. The modified estimators are derived on the basis of regression models for the censoring distribution. A simulation study and a real data example illustrate the results. {\textcopyright} 2006 WILEY-VCH Verlag GmbH & Co. KGaA.},
author = {Gerds, Thomas A. and Schumacher, Martin},
doi = {10.1002/BIMJ.200610301},
issn = {1521-4036},
journal = {Biometrical Journal},
keywords = {Brier Score,Censoring Bias,Inverse of Probability of Censoring Weighting,Model Validation,Survival Analysis},
month = {dec},
number = {6},
pages = {1029--1040},
pmid = {17240660},
publisher = {John Wiley & Sons, Ltd},
title = {{Consistent Estimation of the Expected Brier Score in General Survival Models with Right-Censored Event Times}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/bimj.200610301 https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.200610301 https://onlinelibrary.wiley.com/doi/10.1002/bimj.200610301},
volume = {48},
year = {2006}
}
@misc{Gerds2022,
abstract = {Validation of risk predictions obtained from survival models and competing risk models based on censored data using inverse weighting and cross-validation. Most of the 'pec' functionality has been moved to 'riskRegression'.},
author = {Gerds, Thomas A},
title = {{pec: Prediction Error Curves for Risk Prediction Models in Survival Analysis}},
url = {https://cran.r-project.org/web/packages/pec/index.html},
year = {2022}
}
@misc{Harrell2023,
abstract = {Regression modeling, testing, estimation, validation, graphics, prediction, and typesetting by storing enhanced model design attributes in the fit. 'rms' is a collection of functions that assist with and streamline modeling. It also contains functions for binary and ordinal logistic regression models, ordinal models for continuous Y with a variety of distribution families, and the Buckley-James multiple regression model for right-censored responses, and implements penalized maximum likelihood estimation for logistic and ordinary linear models. 'rms' works with almost any regression model, but it was especially written to work with binary or ordinal regression models, Cox regression, accelerated failure time models, ordinary linear models, the Buckley-James model, generalized least squares for serially or spatially correlated observations, generalized linear models, and quantile regression.},
author = {Harrell, Frank E.},
title = {{rms: Regression Modeling Strategies}},
url = {https://cran.r-project.org/web/packages/rms/index.html},
year = {2023}
}
@misc{Wickham2022,
abstract = {Import excel files into R. Supports '.xls' via the embedded 'libxls' C library <https://github.com/libxls/libxls> and '.xlsx' via the embedded 'RapidXML' C++ library <https://rapidxml.sourceforge.net/>. Works on Windows, Mac and Linux without external dependencies.},
author = {Wickham, Hadley and Bryan, Jennifer and Posit, PBC and Kalicinski, Marcin and Komarov, Valery and Leitienne, Christophe and Colbert, Bob and Hoerl, David and Miller, Evan},
title = {{readxl: Read Excel Files}},
url = {https://cran.r-project.org/web/packages/readxl/index.html},
year = {2022}
}
@article{Strobl2009,
abstract = {Recursive partitioning methods have become popular and widely used tools for nonparametric regression and classification in many scientific fields. Especially random forests, which can deal with large numbers of predictor variables even in the presence of complex interactions, have been applied successfully in genetics, clinical medicine, and bioinformatics within the past few years. High-dimensional problems are common not only in genetics, but also in some areas of psychological research, where only a few subjects can be measured because of time or cost constraints, yet a large amount of data is generated for each subject. Random forests have been shown to achieve a high prediction accuracy in such applications and to provide descriptive variable importance measures reflecting the impact of each variable in both main effects and interactions. The aim of this work is to introduce the principles of the standard recursive partitioning methods as well as recent methodological improvements, to illustrate their usage for low and high-dimensional data exploration, but also to point out limitations of the methods and potential pitfalls in their practical application. Application of the methods is illustrated with freely available implementations in the R system for statistical computing. {\textcopyright} 2009 American Psychological Association.},
author = {Strobl, Carolin and Malley, James and Tutz, Gerhard},
doi = {10.1037/A0016973},
issn = {1082989X},
journal = {Psychological methods},
keywords = {classification,prediction,regression,variable importance},
month = {dec},
number = {4},
pages = {323},
pmid = {19968396},
publisher = {NIH Public Access},
title = {{An Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests}},
url = {/pmc/articles/PMC2927982/ /pmc/articles/PMC2927982/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/},
volume = {14},
year = {2009}
}
@article{Strobl2007,
abstract = {Background: Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories. Results: Simulation studies are presented illustrating that, when random forest variable importance measures are used with data of varying types, the results are misleading because suboptimal predictor variables may be artificially preferred in variable selection. The two mechanisms underlying this deficiency are biased variable selection in the individual classification trees used to build the random forest on one hand, and effects induced by bootstrap sampling with replacement on the other hand. Conclusion: We propose to employ an alternative implementation of random forests, that provides unbiased variable selection in the individual classification trees. When this method is applied using subsampling without replacement, the resulting variable importance measures can be used reliably for variable selection even in situations where the potential predictor variables vary in their scale of measurement or their number of categories. The usage of both random forest algorithms and their variable importance measures in the R system for statistical computing is illustrated and documented thoroughly in an application re-analyzing data from a study on RNA editing. Therefore the suggested method can be applied straightforwardly by scientists in bioinformatics research. {\textcopyright} 2007 Strobl et al; licensee BioMed Central Ltd.},
author = {Strobl, Carolin and Boulesteix, Anne Laure and Zeileis, Achim and Hothorn, Torsten},
doi = {10.1186/1471-2105-8-25/FIGURES/11},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Microarrays},
month = {jan},
number = {1},
pages = {1--21},
pmid = {17254353},
publisher = {BioMed Central},
title = {{Bias in random forest variable importance measures: Illustrations, sources and a solution}},
url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25},
volume = {8},
year = {2007}
}
@article{Hothorn2006,
abstract = {Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to fit such models have been known for a long time: overfitting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overfitting problem, the variable selection bias still seriously affects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a unified framework for recursive partitioning which embeds tree-structured regression models into a well defined theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally different, confirming the need for an unbiased variable selection. Moreover, it is shown thai the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classification, node positive breast cancer survival and mammography experience are re-analyzed. {\textcopyright} 2006 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
doi = {10.1198/106186006X133933},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Multiple testing,Multivariate regression trees,Ordinal regression trees,Permutation tests,Variable selection},
number = {3},
pages = {651--674},
publisher = {Taylor & Francis},
title = {{Unbiased recursive partitioning: A conditional inference framework}},
url = {https://www.tandfonline.com/doi/abs/10.1198/106186006X133933},
volume = {15},
year = {2006}
}
@misc{Hothorn2022,
abstract = {A computational toolbox for recursive partitioning. The core of the package is ctree(), an implementation of conditional inference trees which embed tree-structured regression models into a well defined theory of conditional inference procedures. This non-parametric class of regression trees is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Based on conditional inference trees, cforest() provides an implementation of Breiman's random forests. The function mob() implements an algorithm for recursive partitioning based on parametric models (e.g. linear models, GLMs or survival regression) employing parameter instability tests for split selection. Extensible functionality for visualizing tree-structured regression models is available. The methods are described in Hothorn et al. (2006) <doi:10.1198/106186006X133933>, Zeileis et al. (2008) <doi:10.1198/106186008X319331> and Strobl et al. (2007) <doi:10.1186/1471-2105-8-25>.},
author = {Hothorn, Thorsten and Hornik, Kurt and Strobl, Carolin and Zeileis, Achim},
title = {{party: A Laboratory for Recursive Partytioning}},
url = {https://cran.r-project.org/web/packages/party/index.html},
year = {2022}
}
@article{Ooms2023,
abstract = {Zero-dependency data frame to xlsx exporter based on 'libxlsxwriter'. Fast and no Java or Excel required.},
author = {Ooms, Jeroen and McNamara, John},
month = {jan},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{writexl: Export Data Frames to Excel 'xlsx' Format}},
url = {https://cran.r-project.org/package=writexl},
year = {2023}
}
@misc{Wilke2022,
abstract = {A 'ggplot2' extension that enables the rendering of complex formatted plot labels (titles, subtitles, facet labels, axis labels, etc.). Text boxes with automatic word wrap are also supported.},
author = {Wilke, Claus O and Wiernik, Brenton M},
month = {sep},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{ggtext: Improved Text Rendering Support for 'ggplot2'}},
url = {https://cran.r-project.org/package=ggtext},
year = {2022}
}
@misc{Ripley2022,
abstract = {Functions and datasets to support Venables and Ripley, "Modern Applied Statistics with S" (4th edition, 2002).},
author = {Ripley, Brian},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{MASS: Support Functions and Datasets for Venables and Ripley's MASS}},
url = {https://cran.r-project.org/package=MASS},
year = {2022}
}
@misc{Signorell2022,
abstract = {A collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.},
author = {Signorell, Andri},
month = {oct},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{DescTools: Tools for Descriptive Statistics}},
url = {https://cran.r-project.org/package=DescTools},
year = {2022}
}
@article{Simon2011,
abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of l1 and l2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
doi = {10.18637/JSS.V039.I05},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {Cox model,Elastic net,Lasso,Survival},
month = {mar},
number = {5},
pages = {1--13},
publisher = {American Statistical Association},
title = {{Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v039i05},
volume = {39},
year = {2011}
}
@misc{Harrell2022,
abstract = {Contains many functions useful for data analysis, high-level graphics, utility operations, functions for computing sample size and power, simulation, importing and annotating datasets, imputing missing values, advanced table making, variable clustering, character string manipulation, conversion of R objects to LaTeX and html code, and recoding variables.},
author = {Harrell, Frank E. and Dupont, Charles},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{Hmisc: Harrell Miscellaneous}},
url = {https://cran.r-project.org/package=Hmisc},
year = {2022}
}
@article{Grambsch1994,
abstract = {Nonproportional hazards can often be expressed by extending the Cox model to include time varying coefficients; e.g., for a single covariate, the hazard function for subject i is modelled as exp $\beta$(t)Zi(t). A common example is a treatment effect that decreases with time. We show that the function $\beta$(t) can be directly visualized by smoothing an appropriate residual plot. Also, many tests of proportional hazards, including those of Cox (1972), Gill & Schumacher (1987), Harrell (1986), Lin (1991), Moreau, O'Quigley & Mesbah (1985), Nagelkerke, Oosting & Hart (1984), O'Quigley & Pessione (1989), Schoenfeld (1980) and Wei (1984) are related to time-weighted score tests of the proportional hazards hypothesis, and can be visualized as a weighted least-squares line fitted to the residual plot.},
author = {Grambsch, Patricia M. and Therneau, Terry M.},
doi = {10.2307/2337123},
issn = {00063444},
journal = {Biometrika},
month = {aug},
number = {3},
pages = {515},
publisher = {JSTOR},
title = {{Proportional Hazards Tests and Diagnostics Based on Weighted Residuals}},
volume = {81},
year = {1994}
}
@article{Zou2005,
abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso. {\textcopyright} 2005 Royal Statistical Society.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection},
month = {apr},
number = {2},
pages = {301--320},
title = {{Regularization and variable selection via the elastic net}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2005.00503.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x},
volume = {67},
year = {2005}
}
@misc{Gagolewski2021,
abstract = {A collection of character string/text/natural language processing tools for pattern searching (e.g., with 'Java'-like regular expressions or the 'Unicode' collation algorithm), random string generation, case mapping, string transliteration, concatenation, sorting, padding, wrapping, Unicode normalisation, date-time formatting and parsing, and many more. They are fast, consistent, convenient, and - thanks to 'ICU' (International Components for Unicode) - portable across all locales and platforms.},
author = {Gagolewski, Marek and Tartanus, Bartek},
title = {{Package 'stringi'}},
url = {https://cran.r-project.org/web/packages/stringi/index.html http://cran.ism.ac.jp/web/packages/stringi/stringi.pdf},
year = {2021}
}
@misc{Gohel2022,
abstract = {Create pretty tables for 'HTML', 'PDF', 'Microsoft Word' and 'Microsoft PowerPoint' documents from 'R Markdown'. Functions are provided to let users create tables, modify and format their content. It also extends package 'officer' that does not contain any feature for customized tabular reporting.},
author = {Gohel, David},
title = {{flextable: Functions for Tabular Reporting}},
url = {https://cran.r-project.org/web/packages/flextable/index.html},
year = {2022}
}
@misc{Xie2022,
abstract = {Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.},
author = {Xie, Yihui},
title = {{knitr: A General-Purpose Package for Dynamic Report Generation in R}},
url = {https://cran.r-project.org/web/packages/knitr/index.html},
year = {2022}
}
@misc{Allaire2022,
abstract = {Convert R Markdown documents into a variety of formats.},
author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe},
title = {{rmarkdown: Dynamic Documents for R}},
url = {https://cran.r-project.org/web/packages/rmarkdown/index.html},
year = {2022}
}
@misc{Barnier2022,
abstract = {HTML formats and templates for 'rmarkdown' documents, with some extra features such as automatic table of contents, lightboxed figures, dynamic crosstab helper.},
author = {Barnier, Julien},
title = {{rmdformats: HTML Output Formats and Templates for 'rmarkdown' Documents}},
url = {https://cran.r-project.org/web/packages/rmdformats/index.html},
year = {2022}
}
@misc{Henry2022,
abstract = {A toolbox for working with base types, core R features like the condition system, and core 'Tidyverse' features like tidy evaluation.},
author = {Henry, Lionel and Wickham, Hadley.},
title = {{rlang: Functions for Base Types and Core R and 'Tidyverse' Features}},
url = {https://cran.r-project.org/web/packages/rlang/index.html},
year = {2022}
}
@misc{Kassambara2021,
author = {Kassambara, Alboukadel},
title = {{rstatix: Pipe-Friendly Framework for Basic Statistical Tests}},
url = {https://cran.r-project.org/package=rstatix},
year = {2021}
}
@article{Harrell1996,
abstract = {Multivariable regression models are powerful tools that are used frequently in studies of clinical outcomes. These models can use a mixture of categorical and continuous variables and can handle partially observed (censored) responses. However, uncritical application of modelling techniques can result in models that poorly fit the dataset at hand, or, even more likely, inaccurately predict outcomes on new subjects. One must know how to measure qualities of a model's fit in order to avoid poorly fitted or overfitted models. Measurement of predictive accuracy can be difficult for survival time data in the presence of censoring. We discuss an easily interpretable index of predictive discrimination as well as methods for assessing calibration of predicted survival probabilities. Both types of predictive accuracy should be unbiasedly validated using bootstrapping or cross-validation, before using predictions in a new data series. We discuss some of the hazards of poorly fitted and overfitted regression models and present one modelling strategy that avoids many of the problems discussed. The methods described are applicable to all regression models, but are particularly needed for binary, ordinal, and time-to-event outcomes. Methods are illustrated with a survival analysis in prostate cancer using Cox regression.},
author = {Harrell, Frank E. and Lee, Kerry L. and Mark, Daniel B.},
doi = {10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {pmid:8668867, doi:10.1002/(SICI)1097-0258(19960229},
month = {feb},
number = {4},
pages = {361--387},
pmid = {8668867},
publisher = {Stat Med},
title = {{Multivariable prognostic models: Issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors}},
url = {https://pubmed.ncbi.nlm.nih.gov/8668867/},
volume = {15},
year = {1996}
}
@article{Strobl2008,
abstract = {Background: Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables. Results: We identify two mechanisms responsible for this finding: (i) A preference for the selection of correlated predictors in the tree building process and (ii) an additional advantage for correlated predictor variables induced by the unconditional permutation scheme that is employed in the computation of the variable importance measure. Based on these considerations we develop a new, conditional permutation scheme for the computation of the variable importance measure. Conclusion: The resulting conditional variable importance reflects the true impact of each predictor variable more reliably than the original marginal approach. {\textcopyright} 2008 Strobl et al; licensee BioMed Central Ltd.},
author = {Strobl, Carolin and Boulesteix, Anne Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
doi = {10.1186/1471-2105-9-307},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strobl et al. - 2008 - Conditional variable importance for random forests.pdf:pdf},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Microarrays},
month = {jul},
number = {1},
pages = {1--11},
pmid = {18620558},
publisher = {BioMed Central},
title = {{Conditional variable importance for random forests}},
url = {https://link.springer.com/articles/10.1186/1471-2105-9-307 https://link.springer.com/article/10.1186/1471-2105-9-307},
volume = {9},
year = {2008}
}
@article{Kuhn2008,
abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
author = {Kuhn, Max},
doi = {10.18637/jss.v028.i05},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Model building,NetWorkSpaces,Parallel processing,R,Tuning parameters},
number = {5},
pages = {1--26},
publisher = {American Statistical Association},
title = {{Building predictive models in R using the caret package}},
volume = {28},
year = {2008}
}
@article{Croux2007,
abstract = {The results of a standard principal component analysis (PCA) can be affected by the presence of outliers. Hence robust alternatives to PCA are needed. One of the most appealing robust methods for principal component analysis uses the Projection-Pursuit principle. Here, one projects the data on a lower-dimensional space such that a robust measure of variance of the projected data will be maximized. The Projection-Pursuit-based method for principal component analysis has recently been introduced in the field of chemometrics, where the number of variables is typically large. In this paper, it is shown that the currently available algorithm for robust Projection-Pursuit PCA performs poor in the presence of many variables. A new algorithm is proposed that is more suitable for the analysis of chemical data. Its performance is studied by means of simulation experiments and illustrated on some real data sets. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Croux, C. and Filzmoser, P. and Oliveira, M. R.},
doi = {10.1016/j.chemolab.2007.01.004},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Multivariate statistics,Numerical precision,Optimization,Outliers,Robustness,Scale estimators},
month = {jun},
number = {2},
pages = {218--225},
publisher = {Elsevier},
title = {{Algorithms for Projection-Pursuit robust principal component analysis}},
volume = {87},
year = {2007}
}
@article{Friedman2010,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include l1 (the lasso), l2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
doi = {10.18637/jss.v033.i01},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Hastie, Tibshirani - 2010 - Regularization paths for generalized linear models via coordinate descent.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Coordinate-descent,Elastic net,L1 penalty,Lasso,Logistic regression,Regularization path},
month = {feb},
number = {1},
pages = {1--22},
pmid = {20808728},
publisher = {University of California at Los Angeles},
title = {{Regularization paths for generalized linear models via coordinate descent}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v033i01/v33i01.pdf https://www.jstatsoft.org/index.php/jss/article/view/v033i01},
volume = {33},
year = {2010}
}
@inproceedings{Todorov2013,
abstract = {The main drawback of principal component analysis (PCA) especially for applications in high dimensions is that the extracted components are linear combinations of all input variables. To facilitate the interpretability of PCA various sparse methods have been proposed recently. However all these methods might suffer from the influence of outliers present in the data. An algorithm to compute sparse and robust PCA was recently proposed by Croux et al. We compare this method to standard (non-sparse) classical and robust PCA and several other sparse methods. The considered methods are illustrated on a real data example and compared in a simulation experiment. It is shown that the robust sparse method preserves the sparsity and at the same time provides protection against contamination. {\textcopyright} 2013 Springer-Verlag.},
author = {Todorov, Valentin and Filzmoser, Peter},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-642-33042-1_31},
isbn = {9783642330414},
issn = {21945357},
keywords = {Principcal component analysis,robust statistics},
pages = {283--291},
publisher = {Springer Verlag},
title = {{Comparing classical and robust sparse PCA}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-33042-1_31},
volume = {190 AISC},
year = {2013}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Random forests.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
month = {oct},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Random forests}},
url = {https://link.springer.com/article/10.1023/A:1010933404324},
volume = {45},
year = {2001}
}
@article{Sachs2017,
abstract = {Plots of the receiver operating characteristic (ROC) curve are ubiquitous in medical research. Designed to simultaneously display the operating characteristics at every possible value of a continuous diagnostic test, ROC curves are used in oncology to evaluate screening, diagnostic, prognostic and predictive biomarkers. I reviewed a sample of ROC curve plots from the major oncology journals in order to assess current trends in usage and design elements. My review suggests that ROC curve plots are often ineffective as statistical charts and that poor design obscures the relevant information the chart is intended to display. I describe my new R package that was created to address the shortcomings of existing tools. The package has functions to create informative ROC curve plots, with sensible defaults and a simple interface, for use in print or as an interactive web-based plot. A web application was developed to reach a broader audience of scientists who do not use R.},
author = {Sachs, Michael C.},
doi = {10.18637/jss.v079.c02},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sachs - 2017 - Plotroc A tool for plotting ROC curves.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Graphics,Interactive,Plots,ROC curves},
month = {aug},
number = {1},
pages = {1--19},
publisher = {American Statistical Association},
title = {{Plotroc: A tool for plotting ROC curves}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v079c02/v79c02.pdf https://www.jstatsoft.org/index.php/jss/article/view/v079c02},
volume = {79},
year = {2017}
}
@book{Xie2016,
abstract = {bookdown: Authoring Books and Technical Documents with R Markdown presents a much easier way to write books and technical publications than traditional tools such as LaTeX and Word. The bookdown package inherits the simplicity of syntax and flexibility for data analysis from R Markdown, and extends R Markdown for technical writing, so that you can make better use of document elements such as figures, tables, equations, theorems, citations, and references. Similar to LaTeX, you can number and cross-reference these elements with bookdown. Your document can even include live examples so readers can interact with them while reading the book. The book can be rendered to multiple output formats, including LaTeX/PDF, HTML, EPUB, and Word, thus making it easy to put your documents online. The style and theme of these output formats can be customized. We used books and R primarily for examples in this book, but bookdown is not only for books or R. Most features introduced in this book also apply to other types of publications: journal papers, reports, dissertations, course handouts, study notes, and even novels. You do not have to use R, either. Other choices of computing languages include Python, C, C plus plus, SQL, Bash, Stan, JavaScript, and so on, although R is best supported. You can also leave out computing, for example, to write a fiction. This book itself is an example of publishing with bookdown and R Markdown, and its source is fully available on GitHub.},
author = {Xie, Yihui},
booktitle = {Bookdown: Authoring Books and Technical Documents with R Markdown},
doi = {10.1201/9781315204963},
isbn = {9781351792608},
pages = {1--113},
title = {{Bookdown: Authoring books and technical documents with R Markdown}},
year = {2016}
}
@article{Benjamini1995,
abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses- the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
author = {Benjamini, Yoav and Hochberg, Yosef},
doi = {10.1111/j.2517-6161.1995.tb02031.x},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {bonferroni‐type procedures,familywise error rate,multiple‐comparison procedures,p‐values},
month = {jan},
number = {1},
pages = {289--300},
publisher = {Wiley},
title = {{Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing}},
volume = {57},
year = {1995}
}
@book{Wilke2019,
abstract = {First edition. Intro; Copyright; Table of Contents; Preface; Thoughts on Graphing Software and Figure-Preparation Pipelines; Conventions Used in This Book; Using Code Examples; O'Reilly Online Learning; How to Contact Us; Acknowledgments; Chapter 1. Introduction; Ugly, Bad, and Wrong Figures; Part I. From Data to Visualization; Chapter 2. Visualizing Data: Mapping Data onto Aesthetics; Aesthetics and Types of Data; Scales Map Data Values onto Aesthetics; Chapter 3. Coordinate Systems and Axes; Cartesian Coordinates; Nonlinear Axes; Coordinate Systems with Curved Axes; Chapter 4. Color Scales Color as a Tool to DistinguishColor to Represent Data Values; Color as a Tool to Highlight; Chapter 5. Directory of Visualizations; Amounts; Distributions; Proportions; x-y relationships; Geospatial Data; Uncertainty; Chapter 6. Visualizing Amounts; Bar Plots; Grouped and Stacked Bars; Dot Plots and Heatmaps; Chapter 7. Visualizing Distributions: Histograms and Density Plots; Visualizing a Single Distribution; Visualizing Multiple Distributions at the Same Time; Chapter 8. Visualizing Distributions: Empirical Cumulative Distribution Functions and Q-Q Plots Empirical Cumulative Distribution FunctionsHighly Skewed Distributions; Quantile-Quantile Plots; Chapter 9. Visualizing Many Distributions at Once; Visualizing Distributions Along the Vertical Axis; Visualizing Distributions Along the Horizontal Axis; Chapter 10. Visualizing Proportions; A Case for Pie Charts; A Case for Side-by-Side Bars; A Case for Stacked Bars and Stacked Densities; Visualizing Proportions Separately as Parts of the Total; Chapter 11. Visualizing Nested Proportions; Nested Proportions Gone Wrong; Mosaic Plots and Treemaps; Nested Pies; Parallel Sets Chapter 12. Visualizing Associations Among Two or More Quantitative VariablesScatterplots; Correlograms; Dimension Reduction; Paired Data; Chapter 13. Visualizing Time Series and Other Functions of an Independent Variable; Individual Time Series; Multiple Time Series and Dose-Response Curves; Time Series of Two or More Response Variables; Chapter 14. Visualizing Trends; Smoothing; Showing Trends with a Defined Functional Form; Detrending and Time-Series Decomposition; Chapter 15. Visualizing Geospatial Data; Projections; Layers; Choropleth Mapping; Cartograms Chapter 16. Visualizing UncertaintyFraming Probabilities as Frequencies; Visualizing the Uncertainty of Point Estimates; Visualizing the Uncertainty of Curve Fits; Hypothetical Outcome Plots; Part II. Principles of Figure Design; Chapter 17. The Principle of Proportional Ink; Visualizations Along Linear Axes; Visualizations Along Logarithmic Axes; Direct Area Visualizations; Chapter 18. Handling Overlapping Points; Partial Transparency and Jittering; 2D Histograms; Contour Lines; Chapter 19. Common Pitfalls of Color Use; Encoding Too Much or Irrelevant Information},
address = {Sebastopol},
author = {Wilke, Claus O},
booktitle = {O'Reilly Media},
edition = {1},
isbn = {1492031089},
pages = {389},
publisher = {O'Reilly Media},
title = {{Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures}},
year = {2019}
}
@book{Therneau2000,
address = {New York},
author = {Therneau, Terry M. and Grambsch, Patricia M.},
edition = {1},
isbn = {0-387-98784-3},
publisher = {Springer Verlag},
title = {{Modeling Survival Data: Extending the Cox Model}},
year = {2000}
}
@article{Kassambara2016,
author = {Kassambara, Alboukadel and Kosinski, Marcin and Biecek, Przemyslaw},
title = {{survminer: Drawing Survival Curves using 'ggplot2'}},
url = {https://cran.r-project.org/package=survminer},
year = {2016}
}
@article{Wickham2019,
abstract = {At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next. The tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming. We expect that almost every project will use multiple domain-specific packages outside of the tidyverse: our goal is to provide tooling for the most common challenges; not to solve every possible problem. Notably, the tidyverse doesn't include tools for statistical modelling or communication. These toolkits are critical for data science, but are so large that they merit separate treatment. The tidyverse package allows users to install all tidyverse packages with a single command. There are a number of projects that are similar in scope to the tidyverse. The closest is perhaps Bioconductor (Gentleman et al., 2004; Huber et al., 2015), which provides an ecosystem of packages that support the analysis of high-throughput genomic data.},
author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c{c}}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"{u}}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
doi = {10.21105/joss.01686},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf:pdf},
issn = {2475-9066},
journal = {Journal of Open Source Software},
month = {nov},
number = {43},
pages = {1686},
publisher = {The Open Journal},
title = {{Welcome to the Tidyverse}},
volume = {4},
year = {2019}
}
@book{Wickham2016,
address = {New York},
author = {Wickham, Hadley.},
edition = {1},
isbn = {978-3-319-24277-4},
publisher = {Springer-Verlag},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
url = {https://ggplot2.tidyverse.org},
year = {2016}
}
